# -*- coding: utf-8 -*-
"""LabelEncoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18WCp3rwWXHnA3xewtnswvPyekuOk5jb8
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import numpy as np
import os
from labels import getLabels
from numpy import load
import random
from sklearn.preprocessing import StandardScaler


def getSamples(arr, n):
    return arr[::n-1, :, :]

def scale(seq):
    scalers = {}
    for i in range(seq.shape[1]):
        #scalers[i] = StandardScaler()
        scalers[i] = MinMaxScaler(feature_range=(0, 1))
        seq[:, i, :] = scalers[i].fit_transform(seq[:, i, :])

    return seq

def getDataLSTM(path, batch_size=1):
    root = path
    all_data = []
    batch = 10
    labels_to_idx, _ = getLabels(root)
    files = os.listdir(path = root)
    random.shuffle(files)
    samples = np.array([], dtype=np.float)
    labels = []

    for i, f in enumerate(files):

        label = labels_to_idx[f.partition('_')[0]]
        dict_data = load(root + f)
        data = dict_data['arr_0']
        data = getSamples(data, 2)
        data = data[:37,...]
        data = scale(data)
        data = data.reshape(1,-1,50)
        if i==0:
            samples = data
        else:
            samples = np.vstack( (samples, data))

        labels.append(label)

    labels = np.array(labels)

    train_samples, train_labels = samples[:85,...], labels[:85,...]
    test_samples, test_labels = samples[85:,...], labels[85:,...]

    train_dataset = tf.data.Dataset.from_tensor_slices((train_samples, train_labels))
    train_dataset = train_dataset.shuffle(len(labels))
    train_dataset = train_dataset.batch(batch)

    test_dataset = tf.data.Dataset.from_tensor_slices((test_samples, test_labels))
    test_dataset = test_dataset.shuffle(len(labels))
    test_dataset = test_dataset.batch(batch)

    return train_dataset, test_dataset

def getDataConvLSTM(path, batch_size=1):
    root = path
    all_data = []
    batch = 10
    labels_to_idx, _ = getLabels(root)
    files = os.listdir(path = root)
    random.shuffle(files)
    samples = np.array([], dtype=np.float)
    labels = []

    for i, f in enumerate(files):

        label = labels_to_idx[f.partition('_')[0]]
        dict_data = load(root + f)
        data = dict_data['arr_0']
        data = getSamples(data, 2)
        data = data[:37,...]
        data = scale(data)
        data = data.reshape(1,-1,25,2,1)
        if i==0:
            samples = data
        else:
            samples = np.vstack( (samples, data))

        labels.append(label)

    labels = np.array(labels)

    train_samples, train_labels = samples[:85,...], labels[:85,...]
    test_samples, test_labels = samples[85:,...], labels[85:,...]

    train_dataset = tf.data.Dataset.from_tensor_slices((train_samples, train_labels))
    train_dataset = train_dataset.shuffle(len(labels))
    train_dataset = train_dataset.batch(batch)

    test_dataset = tf.data.Dataset.from_tensor_slices((test_samples, test_labels))
    test_dataset = test_dataset.shuffle(len(labels))
    test_dataset = test_dataset.batch(batch)

    return train_dataset, test_dataset